#!/bin/bash

# Matthew Wyczalkowski <m.wyczalkowski@wustl.edu>
# https://dinglab.wustl.edu/

read -r -d '' USAGE <<'EOF'
Utility for logging, reporting, and cleaning Cromwell run data

Usage:
  datatidy [options] [ RID1 [RID2 ...]]

Required:

Optional options
-h: print usage information
-d: dry run: print commands but do not modify data or write to data log
-1: stop after one case processed.
-l DATALOGD: data log directory or file.  If directory, data log file is DATALOGD/datalog.dat. If path given,
    assumed this is an existing data log file.  Default: is "./logs"
-k CASES_FN: file with list of all cases, one per line, used when CASE1 not defined. Default: dat/cases.dat
-m NOTE: A note added to data log file for each case
-x TASK: Execute given task.  Values: 'query' (default), 'original', 'inputs', 'compress', 'prune', 'final', 'wipe'
-F EXPECTED_STATUS: Define expected status of runs 
-w: output additional detail for query task
-c CQ: explicit path to cromwell query utility `cq`.  Default "cq" 

RID is RunID, and can be either a Case or a WorkflowID.  If RID is - then read
RID from STDIN.  If RID is not defined, read from CASES_FN file.

Datatidy performs the following task:
* `query` - Evaluate and print for each run data directory (values in brackets printed with -w)
    - case, workflowId, tidyLevel, path [, datasize, date, note]
* `original` - Registers run data, marking it as "original" in data log
* Delete and/or compress run data.  In all cases except wipe, CWL outputs will be retained in original path
    * `inputs` - `inputs` subdirectories in all steps deleted.  This takes place for all following tidy levels 
    * `compress` - All data compressed 
    * `prune` - Delete selected intermediate data, retaining compressed logs and key intermediate results
    * `final` - Keep only final CWL outputs of each run
    * `wipe` - Delete everything

Cromwell workflow output can be large and it is useful to reduce disk usage by
deleting staged and intermediate data and compressing other output.  The role
of `datatidy` is to manage and track workflow output deletion.  Tracking is
done using a data log file, and workflow data deletion policy is given
by a "tidy level".  All tasks other than `query` are noted in the data log.

The task `original` marks the run output as having tidy level `original`, i.e.
data are as generated by a completed execution.  By default, only runs with
status (as obtained from `cq`) of `Succeeded` or `Failed` are marked as
`original`, the others are ignored.  To mark runs with some other status as
`original` define EXPTECTED_STATUS.

Tasks which result any deletion of data require EXPECTED_STATUS to be defined.
To help avoid inadvertantly deleting data, all cases must have a status (as
obtained from `cq`) same as EXPECTED_STATUS.

A data log file has the following columns:
    Case        - Case of run
    WorkflowID  - WorkflowID of run
    Path        - Path to workflowRoot directory of this run
    TidyDate    - Date this entry generated (this is not necessarily date of run)
    TidyLevel   - One of: `original`, `inputs`, `compress`, `prune`, `final`, `wipe`
    Note        - Arbitrary note about this run or cleaning

This script relies on `cq` to get WorkflowID, status and timestamps associated with each case

EOF

source cromwell_utils.sh

SCRIPT=$(basename $0)
SCRIPT_PATH=$(dirname $0)

CQ="cq"
CASES_FN="./dat/cases.dat"
NOTE=""
TASK="query"
DATALOGD="./logs"

while getopts ":hd1l:k:m:c:x:wF:" opt; do
  case $opt in
    h) 
      echo "$USAGE"
      exit 0
      ;;
    d)  # echo work command instead of evaluating it
      DRYRUN="d"
      ;;
    1) 
      JUSTONE=1
      ;;
    k) 
      CASES_FN="$OPTARG"
      ;;
    l) 
      DATALOGD="$OPTARG"
      ;;
    m)
      NOTE="$OPTARG"
      ;;
    c) 
      CQ="$OPTARG"
      ;;
    x)
      TASK="$OPTARG"
      ;;
    w) 
      QUERY_DETAIL=1
      ;;
    F) 
      EXPECTED_STATUS="$OPTARG"
      ;;
    \?)
      >&2 echo "Invalid option: -$OPTARG" 
      >&2 echo "$USAGE"
      exit 1
      ;;
    :)
      >&2 echo "Option -$OPTARG requires an argument." 
      >&2 echo "$USAGE"
      exit 1
      ;;
  esac
done
shift $((OPTIND-1))


# this allows us to get Run IDs in one of three ways:
# 1: cq RID1 RID2 ...
# 2: cat cases.dat | cq -
# 3: read from CASES_FN file
# Note that if no Run IDs defined, assume RIDS='-'
if [ "$#" == 0 ]; then
    confirm $CASES_FN
    RIDS=$(cat $CASES_FN)
elif [ "$1" == "-" ] ; then
    RIDS=$(cat - )
else
    RIDS="$@"
fi

# If DATALOGD is an existing directory use that, and create datalog.dat if necessary
# elif DATALOGD is an existing file use that,
# else error
if [ -d $DATALOGD ]; then
    DATALOG="$DATALOGD/datalog.dat"
elif [ -f $DATALOGD ]; then
    DATALOG="$DATALOGD"
else
    >&2 echo ERROR: Not a file or directory: $DATALOGD
    exit 1
fi

if [ ! -f $DATALOG ]; then
    >&2 echo Creating new data log $DATALOG
    # Write header
    printf "Case\tWorkflowID\tPath\tDate\tTidyLevel\tNote\n" > $DATALOG
    test_exit_status
else
    >&2 echo Existing data log $DATALOG
fi

# Given a WorkflowID, return the most recent entry from datalog
# The entire data log line is returned:
#   Case WorkflowID Path Date TidyLevel Note
# If workflowID is not found, return "Unregistered"
function parseDataLog {
    WID=$1
    DATALOG=$2
    confirm $DATALOG

    # Search runlog from bottom, return column 1 of first matching line
    DL=$( tac $DATALOG | grep -m 1 -F "$WID"  )
    test_exit_status
    if [ -z $DL ]; then
        DL="Unregistered"
    fi
    echo "$DL"
}

# Given a run ID (case or WorkflowID), print
# to stdout the following fields: case, workflowId, path, tidyLevel
# If QUERY_DETAIL, also evaluate and print: datasize, date, note
function do_query {
    RID=$1
    # get case and WorkflowID, based on runlog parser in cromwell_utils.sh
    read MYCASE MYWID < <( getCaseWID $RID )
    # get additional details from datalog
    #   Case WorkflowID Path Date TidyLevel Note
    DL=$(parseDataLog $MYWID $DATALOG)
    test_exit_status
    # DL = Unregistered
    if [ "$DL" == "Unregistered" ]; then
        printf "$MYCASE\t$MYWID\tUnregistered\n"
        return
    fi
    DATAD=$( echo "$DL" | cut -f 3 )
    DL_DATE=$( echo "$DL" | cut -f 4 )
    TIDYLEVEL=$( echo "$DL" | cut -f 5 )
    DL_NOTE=$( echo "$DL" | cut -f 6 )

    if [ "$QUERY_DETAIL" ]; then
        DATASIZE=$(du -sh "$DATAD" | cut -f 1)
        test_exit_status
        print "$MYCASE\t$MYWID\t$DATAD\t$TIDYLEVEL\t$DATASIZE\t$DL_DATE\t$DL_NOTE\n" 
    else
        print "$MYCASE\t$MYWID\t$DATAD\t$TIDYLEVEL\n" 
    fi
}

# Evaluate whether status of given RID is as expected.  If not, print error and exit
# Otherwise return WorkflowID
function check_expected_status {
    RID=$1
    ES=$2
    # get case and WorkflowID, based on runlog parser in cromwell_utils.sh
    read MYCASE MYWID < <( getCaseWID $RID )
    STATUS=$( $CQ -V -q status $MYWID )                               
    if [ $STATUS != $ES ]; then
        >&2 echo ERROR: Status of run $RID \( $STATUS \) differs from expected \( $ES \).  Note that \
tasks which modify data require all runs to have status as defined by -F.  No run data has been altered.
        exit 2  # to distinguish status mismatch from garden variety errors
    fi
    return $MYWID
}

# Write entry to datalog file 
# usage: register_datalog $RID $TL
# RID may be a case or a workflow ID.  Lookup is based on getCaseWID in cromwell_utils.sh
function write_datalog {
    RID=$1
    TL=$2

    NOW=$(date)
    # get case and WorkflowID, based on runlog parser in cromwell_utils.sh
    read MYCASE MYWID < <( getCaseWID $RID )
    test_exit_status                                                                   

    DATAD=$( $CQ -V -q workflowRoot $MYWID )                               
    test_exit_status                                                                   

    DATALOGLINE=$( printf "$MYCASE\t$MYWID\t$DATAD\t$NOW\t$TD\t$NOTE\n") 
    if [ "$DRYRUN" ]; then
        >&2 echo Dryrun: data log = "$RL"
    else
        echo "$RL" >> $DATALOG
    fi
    test_exit_status
}

# Register only those runs which have a status of Succeeded or Failed
function register_original {
    RID=$1

    STATUS=$( $CQ -V -q status $RID )                               
    if [ $STATUS == "Succeeded" ] || [ $STATUS == "Failed" ]; then
        write_datalog $RID "original"
    else
        >&2 echo Skipping $RID : Status = $STATUS
    fi
}

# General structure of Cromwell workflow output
# For each run (identified by WorkflowID), there is a directory for each step
# 8f227123-f7e4-4633-ac8d-3cab70483f0d
# ├── call-dbsnp_filter
# │   ├── execution
# │   ├── inputs
# │   └── tmp.5a569257
# ├── call-merge_vcf
# │   ├── execution
# │   ├── inputs
# │   └── tmp.0d4e401f

# Remove inputs directory for all steps
function delete_inputs {
    RID=$1
    WD=$( $CQ -V -q workflowRoot $RID )                               
    if [ ! -d $WD ]; then
        >&2 echo ERROR: WorkflowRoot $WD does not exist
        exit 1
    fi

    CMD="rm -rf $WD/*/inputs"
    run_cmd "$CMD" $DRYRUN
}

function rmrf_exclude {
# Pass 1) root directory, 2) list of files to exclude
# this will use extended pattern matching to remove all directories under root directory
# with the exception of those in exclude list
    >&2 echo Unimplemented
    return 1
}

function do_compress {
# Run tar -zcf run_output.tar.gz *
# Then run rmrf_exclude, keeping final output and run_output.tar.gz
    >&2 echo Unimplemented
    return 1

}

function do_prune {
# TODO: make a list of things we want to keep and delete
#   * All big files, all intermediate files
#   * In general, output of each step should be retained
    >&2 echo Unimplemented
    return 1

}

# Note on deleting: https://www.tecmint.com/delete-all-files-in-directory-except-one-few-file-extensions/
# can use extended pattern matching to not delete given files
function do_final {
# TODO: finish this

# call cq -V outputs to get complete path to all final outputs
# call rmrf_exclude, keeping all final output
    >&2 echo Unimplemented
    return 1

}


function do_wipe {
# run rmrf_exclude but keeping nothing at all.
# them remove root directory
    >&2 echo Unimplemented
    return 1
}

function process_runs {
    TASK=$1
    DOCKET="$2"

    for RID in $DOCKET; do

        case "$TASK" in
          "original_special")
            :   # don't do anything, just let write_dialog below do its thing
            ;;
          "inputs")
            delete_inputs $RID 
            ;;
          "compress")
            delete_inputs $RID 
            do_compress $RID
            ;;
          "prune")
            delete_inputs $RID 
            do_prune $RID
            ;;
          "final")
            delete_inputs $RID 
            do_final $RID
            ;;
          "wipe")
            delete_inputs $RID 
            do_wipe $RID
            ;;
          *)
            >&2 echo ERROR: Unknown task $TASK
            >&2 echo "$USAGE"
            exit 1
            ;;
        esac

        write_datalog $RID $TASK
        test_exit_status
    done
}

# catch original task with a specific status defined
if [ $TASK == "original" ] && [ ! -z "$EXPECTED_STATUS" ]; then
    TASK="original_special"
fi

DO_DOCKET=0
DOCKET=""

# Looping over RunID, which is either Case or WorkflowID
# for cases which involve deleting data (and "original" with non-default status values),
# first check to make sure all runs have expected status before processing them
for RID in $RIDS; do

    case "$TASK" in
      "query")
        do_query $RID 
        ;;
      "original")
        register_original $RID 
        ;;
      "original_special"|"inputs"|"compress"|"prune"|"final"|"wipe")
#        test if workflowID has expected status, and add to docket if so
        DO_DOCKET=1
        if [ -z $EXPECTED_STATUS ]; then
            >&2 echo ERROR: Task $TASK requires EXPECTED_STATUS \(-F\) to be defined
            exit 1
        fi
        WID=$( check_expected_status $RID $EXPECTED_STATUS )
        test_exit_status
        DOCKET="$DOCKET $WID"
        ;;
      *)
        >&2 echo ERROR: Unknown task $TASK
        >&2 echo "$USAGE"
        exit 1
        ;;
    esac

    if [ $JUSTONE ]; then
        break
    fi
done

if [ $DO_DOCKET ]; then

    process_runs $TASK "$DOCKET"

fi



>&2 echo Written to $DATALOG
