#!/bin/bash

# Matthew Wyczalkowski <m.wyczalkowski@wustl.edu>
# https://dinglab.wustl.edu/

read -r -d '' USAGE <<'EOF'
Utility for logging, reporting, and cleaning Cromwell run data

Usage:
  datatidy [options] [ RID1 [RID2 ...]]

Required:

Optional options
-h: print usage information
-d: dry run: print commands but do not modify data or write to data log
-1: stop after one case processed.
-l DATALOGD: data log directory or file.  If directory, data log file is DATALOGD/datalog.dat. If path given,
    assumed this is an existing data log file.  Default: is "./logs"
-k CASES_FN: file with list of all cases, one per line, used when CASE1 not defined. Default: dat/cases.dat
-m NOTE: A note added to data log file for each case
-x TASK: Execute given task.  Values: 'query' (default), 'original', 'inputs', 'compress', 'prune', 'final', 'wipe'
-F EXPECTED_STATUS: Define expected status of runs 
-w: output additional detail for query task
-c CQD: explicit path to cromwell query utility `cq` and `datatidy`

RID is RunID, and can be either a Case or a WorkflowID.  If RID is - then read
RID from STDIN.  If RID is not defined, read from CASES_FN file.

Datatidy performs the following task:
* `query` - Evaluate and print for each run data directory. Values in brackets printed with -w
    - case, workflowId, tidyLevel, path [, datasize, date, note]
* `original` - Registers run data, marking it as "original" in data log
* Delete and/or compress run data.  In all cases except wipe, CWL outputs will be retained in original path
    * `inputs` - `inputs` subdirectories in all steps deleted.  This takes place for all other tidy levels 
    * `compress` - All data compressed 
    * `prune` - Retain and compress final output of each step
    * `final` - Keep only final CWL outputs of each run
    * `wipe` - Delete everything

Cromwell workflow output can be large and it is useful to reduce disk usage by
deleting staged and intermediate data and compressing other output.  The role
of `datatidy` is to manage and track workflow output deletion.  Tracking is
done using a data log file, and workflow data deletion policy is given
by a "tidy level".  All tasks other than `query` are noted in the data log.

The task `original` marks the run output as having tidy level `original`, i.e.
data as generated by a completed execution.  By default, only runs with
status (as obtained from `cq`) of `Succeeded` or `Failed` are marked as
`original`, the others are ignored.  To mark runs with some other status as
`original` define EXPTECTED_STATUS.

Tasks which delete any data require EXPECTED_STATUS to be defined.
To help avoid inadvertantly deleting data, all cases must have a status (as
obtained from `cq`) same as EXPECTED_STATUS.

A data log file has the following columns:
    Case        - Case of run
    WorkflowID  - WorkflowID of run
    Path        - Path to workflowRoot directory of this run
    TidyDate    - Date this entry generated (this is not necessarily date of run)
    TidyLevel   - One of: `original`, `inputs`, `compress`, `prune`, `final`, `wipe`
    Note        - Arbitrary note about this run or cleaning

This script relies on `cq` to get WorkflowID, status and timestamps associated with each case

Note that in the case of queries, only the most recent workflow ID is returned; if there are
multiple runs for a given case the older ones will be masked.  TODO: allow multiple workflow IDs
to be listed for a given case if they exist.

EOF

source cromwell_utils.sh

SCRIPT=$(basename $0)
SCRIPT_PATH=$(dirname $0)

CASES_FN="./dat/cases.dat"
NOTE=""
TASK="query"
DATALOGD="./logs"

if [ -z $CQD ]; then
    CQ="cq"
else
    CQ="$CQD/cq"
fi

while getopts ":hd1l:k:m:c:x:wF:" opt; do
  case $opt in
    h) 
      echo "$USAGE"
      exit 0
      ;;
    d)  # echo work command instead of evaluating it
      DRYRUN="d"
      ;;
    1) 
      JUSTONE=1
      ;;
    k) 
      CASES_FN="$OPTARG"
      ;;
    l) 
      DATALOGD="$OPTARG"
      ;;
    m)
      NOTE="$OPTARG"
      ;;
    c) 
      CQD="$OPTARG"
      ;;
    x)
      TASK="$OPTARG"
      ;;
    w) 
      QUERY_DETAIL=1
      ;;
    F) 
      EXPECTED_STATUS="$OPTARG"
      ;;
    \?)
      >&2 echo "Invalid option: -$OPTARG" 
      >&2 echo "$USAGE"
      exit 1
      ;;
    :)
      >&2 echo "Option -$OPTARG requires an argument." 
      >&2 echo "$USAGE"
      exit 1
      ;;
  esac
done
shift $((OPTIND-1))

# this allows us to get Run IDs in one of three ways:
# 1: cq RID1 RID2 ...
# 2: cat cases.dat | cq -
# 3: read from CASES_FN file
# Note that if no Run IDs defined, assume RIDS='-'
if [ "$#" == 0 ]; then
    confirm $CASES_FN
    RIDS=$(cat $CASES_FN)
elif [ "$1" == "-" ] ; then
    RIDS=$(cat - )
else
    RIDS="$@"
fi

# If DATALOGD is an existing directory use that, and create datalog.dat if necessary
# elif DATALOGD is an existing file use that,
# else error
if [ -d $DATALOGD ]; then
    DATALOG="$DATALOGD/datalog.dat"
elif [ -f $DATALOGD ]; then
    DATALOG="$DATALOGD"
else
    >&2 echo ERROR: Not a file or directory: $DATALOGD
    exit 1
fi

if [ ! -f $DATALOG ]; then
    >&2 echo Creating new data log $DATALOG
    # Write header
    printf "Case\tWorkflowID\tPath\tDate\tTidyLevel\tNote\n" > $DATALOG
    test_exit_status
else
    >&2 echo Existing data log $DATALOG
fi

# Given a WorkflowID, return the most recent entry from datalog
# The entire data log line is returned:
#   Case WorkflowID Path Date TidyLevel Note
# If workflowID is not found, return "Unregistered"
function parseDataLog {
    WID=$1
    DATALOG=$2
    confirm $DATALOG

    # Search runlog from bottom, return column 1 of first matching line
    DL=$( tac $DATALOG | grep -m 1 -F "$WID"  )
    if [ -z "$DL" ]; then
        DL="Unregistered"
    fi
    echo "$DL"
}

# Given a run ID (case or WorkflowID), print
# to stdout the following fields: case, workflowId, path, tidyLevel
# If QUERY_DETAIL, also evaluate and print: datasize, date, note
function do_query {
    RID=$1
    # get case and WorkflowID, based on runlog parser in cromwell_utils.sh
    read MYCASE MYWID < <( getCaseWID $RID )
    # get additional details from datalog
    #   Case WorkflowID Path Date TidyLevel Note
    DL=$(parseDataLog $MYWID $DATALOG)
    test_exit_status
    # DL = Unregistered
    if [ "$DL" == "Unregistered" ]; then
        printf "$MYCASE\t$MYWID\tUnregistered\n"
        return
    fi
    DATAD=$( echo "$DL" | cut -f 3 )
    DL_DATE=$( echo "$DL" | cut -f 4 )
    TIDYLEVEL=$( echo "$DL" | cut -f 5 )
    DL_NOTE=$( echo "$DL" | cut -f 6 )

    if [ "$QUERY_DETAIL" ]; then
        DATASIZE=$(du -sh "$DATAD" | cut -f 1)
        test_exit_status
        printf "$MYCASE\t$MYWID\t$DATAD\t$TIDYLEVEL\t$DATASIZE\t$DL_DATE\t$DL_NOTE\n" 
    else
        printf "$MYCASE\t$MYWID\t$DATAD\t$TIDYLEVEL\n" 
    fi
}

# Evaluate whether status of given RID is as expected.  If not, print error and exit
# Otherwise return WorkflowID
function check_expected_status {
    RID=$1
    ES=$2

    # get case and WorkflowID, based on runlog parser in cromwell_utils.sh
    read MYCASE MYWID < <( getCaseWID $RID )
    STATUS=$( $CQ -V -q status $MYWID )                               
    if [ $STATUS != $ES ]; then
        >&2 echo ERROR: Status of run $RID \( $STATUS \) differs from expected \( $ES \)
        >&2 echo "      " Tasks which delete data require all runs to have expected status \(-F\)
        >&2 echo "      " No run data has been altered
        exit 2  # to distinguish status mismatch from garden variety errors
    fi
    echo "$MYWID"
}

# Write entry to datalog file 
# usage: register_datalog $RID $TL
# RID may be a case or a workflow ID.  Lookup is based on getCaseWID in cromwell_utils.sh
function write_datalog {
    RID=$1
    TL=$2

    NOW=$(date)
    # get case and WorkflowID, based on runlog parser in cromwell_utils.sh
    read MYCASE MYWID < <( getCaseWID $RID )
    test_exit_status                                                                   

    DATAD=$( $CQ -V -q workflowRoot $MYWID )                               
    test_exit_status                                                                   

    DL=$( printf "$MYCASE\t$MYWID\t$DATAD\t$NOW\t$TL\t$NOTE\n") 
    if [ "$DRYRUN" ]; then
        >&2 echo Dryrun: writing to datalog: "$DL"
    else
        echo "$DL" >> $DATALOG
    fi
    test_exit_status
}

# Return a list of CWL outputs of a run
# Pass either workflow ID or case ID
function get_outputs {
    RID=$1
    OUTPUTS=$( $CQ -V -q outputs $RID )                               
    echo "$OUTPUTS" 
}

# Register only those runs which have a status of Succeeded or Failed
function register_original {
    RID=$1

    STATUS=$( $CQ -V -q status $RID )                               
    if [ $STATUS == "Succeeded" ] || [ $STATUS == "Failed" ]; then
        write_datalog $RID "original"
    else
        >&2 echo Skipping datatidy registration $RID : Status = $STATUS
    fi
}

# General structure of Cromwell workflow output
# For each run (identified by WorkflowID), there is a directory for each step
# 8f227123-f7e4-4633-ac8d-3cab70483f0d
# ├── call-dbsnp_filter
# │   ├── execution
# │   ├── inputs
# │   └── tmp.5a569257
# ├── call-merge_vcf
# │   ├── execution
# │   ├── inputs
# │   └── tmp.0d4e401f

# Remove inputs directory for all steps
function delete_inputs {
    RID=$1
    WD=$( $CQ -V -q workflowRoot $RID )                               
    if [ ! -d $WD ]; then
        >&2 echo ERROR: WorkflowRoot $WD does not exist
        exit 1
    fi

    CMD="rm -rf $WD/*/inputs"
    run_cmd "$CMD" $DRYRUN
}

function rmrf_exclude {
    # Pass 1) root directory, 2) whitespace-separated list of files to exclude from deletion, which may include paths or filenames, and may include wildcards
    # All empty directories will also be deleted
    ROOT="$1"
    FILES="$2"

    PATHCMD=""
    # Build up a command string like "-path $F1 -o -path $F2"
    # https://alvinalexander.com/linux-unix/linux-find-multiple-filenames-patterns-command-example
    for F in $FILES; do
        if [ "$PATHCMD" == "" ]; then
            PATHCMD="-path $F"
        else
            PATHCMD="$PATHCMD -o -path $F"
        fi
    done

    # Delete all regular files which are not excluded
    CMD="find $ROOT -type f -not \( $PATHCMD \) -delete"
    run_cmd "$CMD" $DRYRUN

    # Delete all links which are not excluded
    CMD="find $ROOT -type l -not \( $PATHCMD \) -delete"
    run_cmd "$CMD" $DRYRUN

    # Delete all empty directories
    CMD="find $ROOT -type d -empty -delete"
    run_cmd "$CMD" $DRYRUN
}

function do_compress {
    # compress all results to file compressed_results.tar.gz, then remove all 
    # results except final output and compressed_results.tar.gz
    # If the output file already exists skip compressing again, since data loss may occur
    RID=$1

    WD=$( $CQ -V -q workflowRoot $RID )                               

    TAR="compressed_results.tar.gz"
    if [ -f "$WD/$TAR" ]; then
        >&2 echo WARNING: File $WD/$TAR exists.  Will not overwrite, skipping compress task
        return
    fi

    # https://stackoverflow.com/questions/18681595/tar-a-directory-but-dont-store-full-absolute-paths-in-the-archive
    # The `touch` is to avoid error like "tar: .: file changed as we read it" per https://stackoverflow.com/questions/20318852/tar-file-changed-as-we-read-it 
    CMD="touch $WD/$TAR && tar -zcf $WD/$TAR -C $WD --exclude $TAR ."   
    run_cmd "$CMD" $DRYRUN
    
    OUTPUTS=$(get_outputs $RID)
    rmrf_exclude $WD "$OUTPUTS $WD/compressed_results.tar.gz"

    write_datalog $RID $TASK
}

function do_prune {
    # We retain all final outputs of each step (as obtained by inspection of CWL code)
    # we then create pruned_results.tar.gz containing this data
    # Finally, all these files are deleted except for final CWL outputs
    RID=$1

# TODO: update as in compress
#   * tar
#   * check if file exists

    WD=$( $CQ -V -q workflowRoot $RID )                               


    TAR="pruned_results.tar.gz"
    if [ -f "$WD/$TAR" ]; then
        >&2 echo WARNING: File $WD/$TAR exists.  Will not overwrite, skipping prune task
        return
    fi

    RESULTS=" \
        \*/results/dbsnp_filter/dbsnp_pass.vcf \
        \*/results/merged/merged.filtered.vcf \
        \*/results/pindel/filter_out/pindel-raw.dat.CvgVafStrand_pass.Homopolymer_pass.vcf \
        \*/results/varscan/filter_indel_out/varscan.out.som_indel.Somatic.hc.vcf \
        \*/results/varscan/filter_snv_out/varscan.out.som_snv.Somatic.hc.somfilter_pass.vcf \
        \*/results/pindel/pindel_out/pindel-raw.dat \
        \*/results/strelka2/strelka_out/results/variants/somatic.snvs.vcf.gz \
        \*/results/strelka/strelka_out/results/passed.somatic.snvs.vcf \
        \*/results/varscan/varscan_out/varscan.out.som_indel.vcf \
        \*/results/varscan/varscan_out/varscan.out.som_snv.vcf \
        \*/results/vaf_length_depth_filters/filtered.vcf \
        \*/results/maf/output.maf \
        \*/results/vep/output_vep.vcf \
        \*/results/vep_filter/vep_filtered.vcf \
        \*/mutect_call_stats.txt \
        \*/mutect_coverage.wig.txt \
        \*/mutect.vcf \
    "
    OUTPUTS=$(get_outputs $RID)
    # Delete all except results of each step
    rmrf_exclude $WD "$OUTPUTS $RESULTS"

    # https://stackoverflow.com/questions/18681595/tar-a-directory-but-dont-store-full-absolute-paths-in-the-archive
    # The `touch` is to avoid error like "tar: .: file changed as we read it" per https://stackoverflow.com/questions/20318852/tar-file-changed-as-we-read-it 
    CMD="touch $WD/$TAR && tar -zcf $WD/$TAR -C $WD --exclude $TAR ."   
    run_cmd "$CMD" $DRYRUN
    
    # Finally remove all except final output and the .tar.gz file
    rmrf_exclude $WD "$OUTPUTS \*/$TAR"
    write_datalog $RID $TASK
}

# Note on deleting: https://www.tecmint.com/delete-all-files-in-directory-except-one-few-file-extensions/
# can use extended pattern matching to not delete given files
function do_final {
    RID=$1

    WD=$( $CQ -V -q workflowRoot $RID )                               
    
    OUTPUTS=$(get_outputs $RID)
    rmrf_exclude $WD "$OUTPUTS "

}


# Bad and scary - remove entire workflow directory
function do_wipe {
    RID=$1

    WD=$( $CQ -V -q workflowRoot $RID )                               
    CMD="rm -rf $WD"
    run_cmd "$CMD" $DRYRUN
    write_datalog $RID $TASK
}

# catch original task with a specific status defined
if [ $TASK == "original" ] && [ ! -z "$EXPECTED_STATUS" ]; then
    TASK="original_special"
fi

# for cases which involve deleting data (and "original" with non-default status values),
# first check to make sure all runs have expected status before processing them
if [ $TASK == "original_special" ] || [ $TASK == "inputs" ] || [ $TASK == "compress" ] || \
   [ $TASK == "prune" ] || [ $TASK == "final" ] || [ $TASK == "wipe" ]; then

    # if expecting a given status, first pre-check to make sure all cases are of this status
    # to do this.  Note this is alternative logic to datatidy, where testing during main loop
    if [ -z $EXPECTED_STATUS ]; then
        >&2 echo ERROR: Task $TASK requires EXPECTED_STATUS \(-F\) to be defined
        exit 1
    fi

    DOCKET=""
    for RID in $RIDS; do
        WID=$( check_expected_status $RID $EXPECTED_STATUS )
        test_exit_status

        DOCKET="$DOCKET $WID"
        if [ $JUSTONE ]; then
            break
        fi
    done
    RIDS=$DOCKET    #  CASE replenished to loop through all cases again
fi

# Looping over RunID, which is either Case or WorkflowID
for RID in $RIDS; do

    case "$TASK" in
      "query")
        do_query $RID 
        ;;
      "original")
        register_original $RID 
        ;;
      "original_special")
        write_datalog $RID $TASK
        ;;
      "inputs")
        delete_inputs $RID 
        write_datalog $RID $TASK
        ;;
      "compress")
        delete_inputs $RID 
        do_compress $RID
        ;;
      "prune")
        delete_inputs $RID 
        do_prune $RID
        ;;
      "final")
        delete_inputs $RID 
        do_final $RID
        ;;
      "wipe")
        delete_inputs $RID 
        do_wipe $RID
        ;;
      *)
        >&2 echo ERROR: Unknown task $TASK
        >&2 echo "$USAGE"
        exit 1
        ;;
    esac

    if [ $JUSTONE ]; then
        break
    fi
done

